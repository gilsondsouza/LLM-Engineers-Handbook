#!/usr/bin/env python3
"""
Demo: Consultando o arquivo extraÃ­do usando LLM
"""

import json
from datetime import datetime
from pathlib import Path

class SimpleLLMQuery:
    """Sistema simples de consulta baseado no conteÃºdo extraÃ­do"""
    
    def __init__(self, chunks_file="data/artifacts/documento_chunks.json"):
        self.chunks = self.load_chunks(chunks_file)
        
    def load_chunks(self, chunks_file):
        """Carrega os chunks do arquivo processado"""
        try:
            with open(chunks_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"âŒ Arquivo nÃ£o encontrado: {chunks_file}")
            return []
    
    def search_content(self, query):
        """Busca conteÃºdo relevante baseado na consulta"""
        query_lower = query.lower()
        relevant_chunks = []
        
        for chunk in self.chunks:
            # Busca simples por palavras-chave
            title_lower = chunk['title'].lower()
            content_lower = chunk['content'].lower()
            
            score = 0
            
            # PontuaÃ§Ã£o baseada em palavras-chave
            keywords = query_lower.split()
            for keyword in keywords:
                if keyword in title_lower:
                    score += 3  # TÃ­tulo tem peso maior
                if keyword in content_lower:
                    score += 1
            
            if score > 0:
                chunk_result = chunk.copy()
                chunk_result['relevance_score'] = score
                relevant_chunks.append(chunk_result)
        
        # Ordenar por relevÃ¢ncia
        relevant_chunks.sort(key=lambda x: x['relevance_score'], reverse=True)
        return relevant_chunks
    
    def generate_response(self, query, max_chunks=3):
        """Gera resposta baseada nos chunks mais relevantes"""
        relevant_chunks = self.search_content(query)
        
        if not relevant_chunks:
            return "âŒ NÃ£o encontrei informaÃ§Ãµes relevantes para sua consulta."
        
        # Construir resposta
        response = f"ğŸ“‹ **Resposta para: {query}**\n\n"
        
        for i, chunk in enumerate(relevant_chunks[:max_chunks], 1):
            response += f"**{i}. {chunk['title']}** (RelevÃ¢ncia: {chunk['relevance_score']})\n"
            response += f"{chunk['content']}\n\n"
        
        # Adicionar informaÃ§Ãµes de contexto
        response += f"---\n"
        response += f"ğŸ” Consulta processada em: {datetime.now().strftime('%H:%M:%S')}\n"
        response += f"ğŸ“Š Chunks analisados: {len(self.chunks)}\n"
        response += f"âœ… Chunks relevantes: {len(relevant_chunks)}\n"
        
        return response

def demo_queries():
    """Demonstra consultas ao sistema"""
    
    print("ğŸ¤– DEMO: Consultando arquivo extraÃ­do com LLM")
    print("=" * 60)
    
    # Inicializar sistema de consulta
    llm_query = SimpleLLMQuery()
    
    if not llm_query.chunks:
        print("âŒ Nenhum chunk encontrado. Execute primeiro o processo de extraÃ§Ã£o.")
        return
    
    # Consultas de demonstraÃ§Ã£o
    demo_questions = [
        "Quais tipos de arquivo sÃ£o suportados?",
        "Como funciona o processamento?",
        "Quais tecnologias sÃ£o utilizadas?",
        "Como fazer upload de arquivos?"
    ]
    
    for question in demo_questions:
        print(f"\nğŸ” **CONSULTA:** {question}")
        print("-" * 50)
        
        response = llm_query.generate_response(question)
        print(response)
        
        print("=" * 60)

def interactive_mode():
    """Modo interativo para consultas"""
    
    print("\nğŸ¯ MODO INTERATIVO")
    print("Digite suas perguntas sobre o documento extraÃ­do.")
    print("Digite 'sair' para encerrar.\n")
    
    llm_query = SimpleLLMQuery()
    
    if not llm_query.chunks:
        print("âŒ Nenhum chunk encontrado.")
        return
    
    while True:
        try:
            query = input("â“ Sua pergunta: ").strip()
            
            if query.lower() in ['sair', 'exit', 'quit']:
                print("ğŸ‘‹ AtÃ© logo!")
                break
            
            if not query:
                continue
            
            print("\nğŸ¤– Processando...")
            response = llm_query.generate_response(query)
            print(f"\n{response}\n")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Saindo...")
            break
        except Exception as e:
            print(f"âŒ Erro: {str(e)}")

def show_document_info():
    """Mostra informaÃ§Ãµes sobre o documento carregado"""
    
    try:
        with open("data/artifacts/documento_chunks.json", 'r', encoding='utf-8') as f:
            chunks = json.load(f)
        
        print("ğŸ“„ INFORMAÃ‡Ã•ES DO DOCUMENTO")
        print("=" * 40)
        print(f"ğŸ“Š Total de seÃ§Ãµes: {len(chunks)}")
        print(f"ğŸ†” ID do documento: {chunks[0]['document_id'] if chunks else 'N/A'}")
        
        total_words = sum(chunk['word_count'] for chunk in chunks)
        total_chars = sum(chunk['char_count'] for chunk in chunks)
        
        print(f"ğŸ“ Total de palavras: {total_words}")
        print(f"ğŸ“„ Total de caracteres: {total_chars}")
        
        print("\nğŸ“š SEÃ‡Ã•ES DISPONÃVEIS:")
        for i, chunk in enumerate(chunks, 1):
            print(f"   {i}. {chunk['title']} ({chunk['word_count']} palavras)")
        
    except FileNotFoundError:
        print("âŒ Documento nÃ£o encontrado. Execute primeiro a extraÃ§Ã£o.")

def main():
    """FunÃ§Ã£o principal"""
    
    print("ğŸš€ LLM Query System - Arquivo ExtraÃ­do")
    print("=" * 50)
    
    # Mostrar informaÃ§Ãµes do documento
    show_document_info()
    
    print("\nEscolha uma opÃ§Ã£o:")
    print("1. ğŸ¬ Demo com consultas prÃ©-definidas")
    print("2. ğŸ¯ Modo interativo")
    print("3. âŒ Sair")
    
    try:
        choice = input("\nSua escolha (1-3): ").strip()
        
        if choice == "1":
            demo_queries()
        elif choice == "2":
            interactive_mode()
        elif choice == "3":
            print("ğŸ‘‹ AtÃ© logo!")
        else:
            print("âŒ OpÃ§Ã£o invÃ¡lida")
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Saindo...")

if __name__ == "__main__":
    main()
